{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic and linear regression with deterministic and stochastic first order methods\n",
    "\n",
    "    TP 3 : Optimisation - DataScience Master\n",
    "    Authors : St√©phane Gaiffas, Alexandre Gramfort\n",
    "   \n",
    "The aim of this TP is to implement and compare various batch and stochastic algorithms for linear and logistic with ridge penalization. \n",
    "The following methods are compared in this notebook.\n",
    "\n",
    "**Batch (deterministic) methods**\n",
    "- gradient descent (GD)\n",
    "- accelerated gradient descent (AGD)\n",
    "- L-BFGS\n",
    "- conjugate gradient (CG)\n",
    "\n",
    "**Stochastic algorithms**\n",
    "\n",
    "- stochastic gradient descent (SGD)\n",
    "- stochastic averaged gradient (SAG)\n",
    "- stochastic variance reduced gradient (SVRG)\n",
    "\n",
    "Note that we consider as use-cases logistic and linear regression with ridge penalization only, although most of the algorithms below can be used with many other models, and other types of penalization, eventually non-smooth ones, such as the $\\ell_1$ penalization.\n",
    "\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "- This work **must be done by pairs of students**.\n",
    "- **Each** student must send their work **before the 27th of november at 23:55**, using the **moodle platform**.\n",
    "- This means that **each student in the pair sends the same file**\n",
    "- On the moodle, in the \"Optimization for Data Science\" course, you have a section called **2016/11/21 Practical session **. This is where you submit your jupyter notebook file. \n",
    "- The **name of the file must be** constructed as in the next cell\n",
    "\n",
    "### How to construct the name of your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp3_masset_camille_and_muzellec_boris.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Change here using YOUR first and last names\n",
    "fn1 = \"camille\"\n",
    "ln1 = \"masset\"\n",
    "fn2 = \"boris\"\n",
    "ln2 = \"muzellec\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"tp3\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gentle reminder: no evaluation if you don't respect this EXACTLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of content\n",
    "\n",
    "[1. Loss functions, gradients and step-sizes](#loss)<br>\n",
    "[2. Generate a dataset](#data)<br>\n",
    "[3. Deterministic methods](#batch)<br>\n",
    "[4. Stochastic methods](#stoc)<br>\n",
    "[5. Numerical comparison](#comp)<br>\n",
    "[6. Conclusion](#conc)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import norm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='loss'></a>\n",
    "## 1. Loss functions, gradients and step-sizes\n",
    "\n",
    "We want to minimize\n",
    "$$\n",
    "\\frac 1n \\sum_{i=1}^n \\ell(x_i^\\top \\theta, y_i) + \\frac \\lambda 2 \\|\\theta\\|_2^2\n",
    "$$\n",
    "where\n",
    "- $\\ell(z, y) = \\frac{1}{2} (y - z)^2$ (least-squares regression)\n",
    "- $\\ell(z, y) = \\log(1 + \\exp(-yz))$ (logistic regression).\n",
    "\n",
    "We write it as a a minimization problem of the form\n",
    "$$\n",
    "\\frac 1n \\sum_{i=1}^n f_i(\\theta)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "f_i(\\theta) = \\ell(x_i^\\top \\theta, y_i) + \\frac \\lambda 2 \\|\\theta\\|_2^2.\n",
    "$$\n",
    "\n",
    "For both cases, the gradients are\n",
    "$$\n",
    "\\nabla f_i(\\theta) = (x_i^\\top \\theta - y_i) x_i + \\lambda \\theta\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\nabla f_i(\\theta) = - \\frac{y_i}{1 + \\exp(y_i x_i^\\top \\theta)} x_i + \\lambda \\theta.\n",
    "$$\n",
    "\n",
    "Denote by $L$ the Lipschitz constant of $f$ and by $L_i$ the Lipschitz constant of $f_i$. \n",
    "One can see easily that for linear regression\n",
    "$$\n",
    "L = \\frac{ \\|\\mathbf X^\\top \\mathbf X \\|_{\\text{op}}}{n} + \\lambda \\quad \\text{ and } L_i = \\| x_i \\|_2^2 + \\lambda\n",
    "$$\n",
    "while for logistic regression it is\n",
    "$$\n",
    "L = \\frac{ \\|\\mathbf X^\\top \\mathbf X \\|_{\\text{op}}}{4 n} + \\lambda \\quad \\text{ and } L_i = \\frac 14 \\| x_i \\|_2^2 + \\lambda.\n",
    "$$\n",
    "For full-gradient methods, the theoretical step-size is $1 / L$, while for SAG and SVRG (see below) it can be taken as\n",
    "$1 / \\max_{i=1,\\ldots,n} L_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now introduce a class that will be used for the solvers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import svd\n",
    "\n",
    "class LinReg(object):\n",
    "    \"\"\"A class for the least-squares regression with\n",
    "    Ridge penalization\"\"\"\n",
    "\n",
    "    def __init__(self, A, b, lbda):\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.n, self.d = A.shape\n",
    "        self.lbda = lbda\n",
    "    \n",
    "    def grad(self, x):\n",
    "        return self.A.T.dot(A.dot(x) - self.b) / self.n + self.lbda * x\n",
    "\n",
    "    def loss(self, x):\n",
    "        return norm(self.A.dot(x) - self.b) ** 2 / (2. * self.n) + self.lbda * norm(x) ** 2 / 2.\n",
    "    \n",
    "    def grad_i(self, i, x):\n",
    "        a_i = self.A[i]\n",
    "        return (a_i.dot(x) - self.b[i]) * a_i + self.lbda * x\n",
    "\n",
    "    def lipschitz_constant(self):\n",
    "        \"\"\"Return the Lipschitz constant of the gradient\"\"\"\n",
    "        L = np.max(svd(self.A, full_matrices=False)[1]) ** 2 / self.n + self.lbda\n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogReg(object):\n",
    "    \"\"\"A class for the logistic regression with L2 penalization\"\"\"\n",
    "\n",
    "    def __init__(self, A, b, lbda):\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.n, self.d = A.shape\n",
    "        self.lbda = lbda\n",
    "    \n",
    "    def grad(self, x):\n",
    "        bAx = self.b * np.dot(self.A, x)\n",
    "        temp = 1. / (1. + np.exp(bAx))\n",
    "        grad = - np.dot(self.A.T, self.b * temp) / self.n + self.lbda * x\n",
    "        return grad\n",
    "\n",
    "    def loss(self, x):\n",
    "        bAx = self.b * np.dot(self.A, x)\n",
    "        return np.mean(np.log(1. + np.exp(- bAx))) + self.lbda * norm(x) ** 2 / 2.\n",
    "    \n",
    "    def grad_i(self, i, x):\n",
    "        grad = - self.A[i] * self.b[i] / (1. + np.exp(self.b[i] * np.dot(self.A[i], x)))\n",
    "        grad += self.lbda * x\n",
    "        return grad\n",
    "\n",
    "    def lipschitz_constant(self):\n",
    "        \"\"\"Return the Lipschitz constant of the gradient\"\"\"\n",
    "        L = np.sum(np.max(self.A ** 2, axis=1)) / (4. * self.n) + self.lbda\n",
    "        return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "## 2. Generate a dataset\n",
    "\n",
    "We generate datasets for the least-squares and the logistic cases. First we define a function for the least-squares case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal, randn\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "\n",
    "    \n",
    "def simu_linreg(x, n, std=1., corr=0.5):\n",
    "    \"\"\"\n",
    "    Simulation of the least-squares problem\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray, shape=(d,)\n",
    "        The coefficients of the model\n",
    "    \n",
    "    n : int\n",
    "        Sample size\n",
    "    \n",
    "    std : float, default=1.\n",
    "        Standard-deviation of the noise\n",
    "\n",
    "    corr : float, default=0.5\n",
    "        Correlation of the features matrix\n",
    "    \"\"\"    \n",
    "    d = x.shape[0]\n",
    "    cov = toeplitz(corr ** np.arange(0, d))\n",
    "    A = multivariate_normal(np.zeros(d), cov, size=n)\n",
    "    noise = std * randn(n)\n",
    "    b = A.dot(x_truth) + noise\n",
    "    return A, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simu_logreg(x, n, std=1., corr=0.5):\n",
    "    \"\"\"\n",
    "    Simulation of the logistic regression problem\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray, shape=(d,)\n",
    "        The coefficients of the model\n",
    "    \n",
    "    n : int\n",
    "        Sample size\n",
    "    \n",
    "    std : float, default=1.\n",
    "        Standard-deviation of the noise\n",
    "\n",
    "    corr : float, default=0.5\n",
    "        Correlation of the features matrix\n",
    "    \"\"\"    \n",
    "    A, b = simu_linreg(x, n, std=1., corr=0.5)\n",
    "    return A, np.sign(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = 50\n",
    "n = 10000\n",
    "idx = np.arange(d)\n",
    "\n",
    "# Ground truth coefficients of the model\n",
    "x_truth = (-1) ** (idx - 1) * np.exp(-idx / 10.)\n",
    "\n",
    "A, b = simu_linreg(x_truth, n, std=1., corr=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhQAAAFkCAYAAAB4sKK5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZIyM1BIkqTMDBSSJCmz/w8h0toxmvZbuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106bc8c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.stem(x_truth);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerically check loss and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0146480800828354e-05"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import check_grad\n",
    "\n",
    "lbda = 1. / n ** (0.5)\n",
    "model = LinReg(A, b, lbda)\n",
    "\n",
    "# Check that the gradient and the loss numerically match\n",
    "check_grad(model.loss, model.grad, np.random.randn(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7030657178890007e-07"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbda = 1. / n ** (0.5)\n",
    "model = LogReg(A, b, lbda)\n",
    "\n",
    "# Check that the gradient and the loss numerically match\n",
    "check_grad(model.loss, model.grad, np.random.randn(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = LinReg(A, b, lbda)\n",
    "model = LogReg(A, b, lbda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the theoretical step-size for FISTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0.673193036786\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import svd\n",
    "\n",
    "step = 1. / model.lipschitz_constant()\n",
    "\n",
    "print(\"step = %s\" % step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a very precise minimum to compute distances to minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.477144493221\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "x_init = np.zeros(d)\n",
    "x_min, f_min, _ = fmin_l_bfgs_b(model.loss, x_init, model.grad, pgtol=1e-20)\n",
    "print(f_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='batch'></a> \n",
    "\n",
    "## 3. Deterministic methods (ISTA, FISTA, BGFS)\n",
    "\n",
    "Before implementing the logic of GD, ISTA or FISTA we provide a simple function to be called after each iteration to gather and display a few metrics about current the minimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inspector(loss_fun, x_real, verbose=False):\n",
    "    \"\"\"A closure called to update metrics after each iteration.\"\"\"\n",
    "    objectives = []\n",
    "    errors = []\n",
    "    it = [0] # This is a hack to be able to modify 'it' inside the closure.\n",
    "    def inspector_cl(xk):\n",
    "        obj = loss_fun(xk) - f_min\n",
    "        err = norm(xk - x_min)\n",
    "        objectives.append(obj)\n",
    "        errors.append(err)\n",
    "        if verbose == True:\n",
    "            if it[0] == 0:\n",
    "                print(' | '.join([name.center(8) for name in [\"it\", \"obj\", \"err\"]]))\n",
    "            if it[0] % (n_iter / 5) == 0:\n",
    "                print(' | '.join([(\"%d\" % it[0]).rjust(8), (\"%.2e\" % obj).rjust(8), (\"%.2e\" % err).rjust(8)]))\n",
    "            it[0] += 1\n",
    "    inspector_cl.obj = objectives\n",
    "    inspector_cl.err = errors\n",
    "    return inspector_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of iterations\n",
    "n_iter = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISTA\n",
    "\n",
    "We recall that an iteration of ISTA (actually a batch gradient here) writes\n",
    "\n",
    "$$\n",
    "x_{k+1} \\gets x_k - \\eta \\nabla f(x_k)\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the step-size (that can be chosen in theory as $\\eta = 1 / L$, with $L$ the Lipshitz constant of $\\nabla f$, see above)\n",
    "\n",
    "*QUESTION*:\n",
    "- Fill in the iteration of the Ista solver in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ista(x_init, grad, n_iter=100, step=1., callback=None):\n",
    "    \"\"\"ISTA algorithm.\"\"\"\n",
    "    x = x_init.copy()\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        x = x - step * grad(x)\n",
    "\n",
    "        # Update metrics after each iteration.\n",
    "        if callback: \n",
    "            callback(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "step = 1. / model.lipschitz_constant()\n",
    "x_init = np.zeros(d)\n",
    "ista_inspector = inspector(model.loss, x_truth, verbose=True)\n",
    "x_ista = ista(x_init, model.grad, n_iter, step, callback=ista_inspector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FISTA\n",
    "\n",
    "We recall that an iteration of FISTA (actually an accelerated batch gradient here) writes\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_{k+1} &\\gets y_k - \\eta \\nabla f(y_k) \\\\\n",
    "t_{k+1} &\\gets \\frac{1 + \\sqrt{1 + 4 t_k^2}}{2} \\\\\n",
    "y_{k+1} &\\gets x_{k+1} + \\frac{t_k-1}{t_{k+1}} (x_{k+1} - x_k)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the step-size (that can be chosen in theory as $\\eta = 1 / L$, with $L$ the Lipshitz constant of $\\nabla f$, see above)\n",
    "\n",
    "*QUESTION*:\n",
    "- Fill in the iteration of the Fista solver in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fista(x_init, grad, n_iter=100, step=1., callback=None):\n",
    "    \"\"\"FISTA algorithm.\"\"\"\n",
    "    x = x_init.copy()\n",
    "    y = x_init.copy()\n",
    "    t = 1.\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        x_new = y - step * grad(y)\n",
    "        t_new = (1 + np.sqrt(1 + 4*t*t)) / 2\n",
    "        y_new = x_new + (t - 1) / t_new * (x_new - x)\n",
    "        x, y, t = x_new, y_new, t_new\n",
    "\n",
    "        # Update metrics after each iteration.\n",
    "        if callback: \n",
    "            callback(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "step = 1. / model.lipschitz_constant()\n",
    "x_init = np.zeros(d)\n",
    "fista_inspector = inspector(model.loss, x_truth, verbose=True)\n",
    "x_fista = fista(x_init, model.grad, n_iter, step, callback=fista_inspector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scipy.optimize's conjuguate gradient\n",
    "\n",
    "Let's compare with ``scipy.optimize``'s conjuguate gradient solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Conjugate gradient descent\n",
    "from scipy.optimize import fmin_cg\n",
    "\n",
    "x_init = np.zeros(d)\n",
    "cg_inspector = inspector(model.loss, x_truth, verbose=True)\n",
    "x_cg = fmin_cg(model.loss, x_init, model.grad, maxiter=n_iter, callback=cg_inspector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scipy.optimize's BFGS\n",
    "\n",
    "Let's compare with ``scipy.optimize``'s BFGS solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Conjugate gradient descent\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "x_init = np.zeros(d)\n",
    "bfgs_inspector = inspector(model.loss, x_truth, verbose=True)\n",
    "x_bfgs, _, _ = fmin_l_bfgs_b(model.loss, x_init, model.grad, maxiter=n_iter, callback=bfgs_inspector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first numerical comparison of deterministic solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "inspectors = [ista_inspector, fista_inspector, cg_inspector, bfgs_inspector]\n",
    "\n",
    "solvers = [\"ISTA\", \"FISTA\", \"CG\", \"BGFS\"]\n",
    "\n",
    "for insp in inspectors:\n",
    "    plt.plot(insp.obj, lw=2)\n",
    "    plt.title(\"Loss\", fontsize=18)\n",
    "    plt.xlabel(\"iteration\", fontsize=14)\n",
    "    plt.ylabel(\"objective\", fontsize=14)\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "plt.legend(solvers)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "for insp in inspectors:\n",
    "    plt.plot(insp.err, lw=2)\n",
    "    plt.title(\"Loss\", fontsize=18)\n",
    "    plt.xlabel(\"iteration\", fontsize=14)\n",
    "    plt.ylabel(\"distance\", fontsize=14)\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "plt.legend(solvers);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "xs = [x_ista, x_fista, x_cg, x_bfgs]\n",
    "\n",
    "for i, name, x in zip(range(1, 5), solvers, xs):\n",
    "    plt.subplot(1, 4, i)\n",
    "    plt.stem(x)\n",
    "    plt.ylim([-1, 1])\n",
    "    plt.title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First conclusions\n",
    "\n",
    "*QUESTIONS*:\n",
    "\n",
    "- Give some first conclusions about the batch solver studied here\n",
    "- What do you observe about Fista, is it suprising ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Answers_**\n",
    "* First, we notice that the second order algorithms (L-BFGS) and Conjugate Gradient perform far better than ISTA and FISTA. L-BFGS and CG perform quite similarly. At 20 iterations, they give better results than FISTA by 4 orders of magnitude, and better than ISTA by 6 order of magnitude (objective loss). \n",
    "* We clearly see that the loss does not monotonously decreasing but it has a bumpy behaviour: it is not a descent algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stoc'></a> \n",
    "## 4. Stochastic methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_iter = 50\n",
    "\n",
    "# generate indices of random samples\n",
    "iis = np.random.randint(0, n, n * n_iter)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD\n",
    "\n",
    "We recall that an iteration of SGD writes\n",
    "\n",
    "- Pick $i$ uniformly at random in $\\{1, \\ldots, n\\}$\n",
    "- Apply\n",
    "$$\n",
    "x_{t+1} \\gets x_t - \\frac{\\eta_0}{\\sqrt{t+1}} \\nabla f_i(x_t)\n",
    "$$\n",
    "\n",
    "where $\\eta_0$ is a step-size to be tuned by hand.\n",
    "\n",
    "*QUESTION*:\n",
    "- Fill in the iteration of the SGD solver in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd(x_init, iis, grad_i, n_iter=100, step=1., callback=None):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    x = x_init.copy()\n",
    "    \n",
    "    for idx in range(n_iter):\n",
    "        i = iis[idx]\n",
    "        \n",
    "        x = x - step / np.sqrt(idx+1) * grad_i(i, x)\n",
    "        \n",
    "        # Update metrics after each iteration.\n",
    "        if callback and idx % n == 0:\n",
    "            callback(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step0 = 0.1\n",
    "x_init = np.zeros(d)\n",
    "sgd_inspector = inspector(model.loss, x_truth, verbose=True)\n",
    "x_sgd = sgd(x_init, iis, model.grad_i, n * n_iter, step=step0, callback=sgd_inspector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAG\n",
    "\n",
    "We recall that an iteration of SAG writes\n",
    "\n",
    "For $t=1, \\ldots, $ until convergence\n",
    "\n",
    "1. Pick $i_t$ uniformly at random in $\\{1, \\ldots, n\\}$\n",
    "\n",
    "2. Update the average of gradients\n",
    "$$\n",
    "G_t \\gets \\frac 1n \\sum_{i=1}^n g_i^t\n",
    "$$\n",
    "where \n",
    "$$\n",
    "g_i^t =\n",
    "\\begin{cases}\n",
    "    \\nabla f_{i}(x_t) &\\text{ if } i = i_t \\\\\n",
    "    g_i^{t-1} & \\text{ otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "3. Apply the step \n",
    "$$x_{t+1} \\gets x_t - \\eta G_t$$\n",
    "where $\\eta$ is the step-size (see code below).\n",
    "\n",
    "*QUESTION*:\n",
    "- Fill in the iteration of the SAG solver in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sag(x_init, iis, grad_i, n_iter=100, step=1., callback=None):\n",
    "    \"\"\"Stochastic average gradient algorithm.\"\"\"\n",
    "    x = x_init.copy()\n",
    "    # Old gradients\n",
    "    gradient_memory = np.zeros((n, d))\n",
    "    y = np.zeros(d)\n",
    "\n",
    "    for idx in range(n_iter):\n",
    "        i = iis[idx]\n",
    "        \n",
    "        g = grad_i(i, x)\n",
    "        y += (g - gradient_memory[i, :]) / n\n",
    "        gradient_memory[i, :] = g\n",
    "        x = x - step * y\n",
    "        \n",
    "        # Update metrics after each iteration.\n",
    "        if callback and idx % n == 0:\n",
    "            callback(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_squared_sum = np.max(np.sum(model.A ** 2, axis=1))\n",
    "\n",
    "if isinstance(model, LogReg):\n",
    "    step = 4.0 / (max_squared_sum + 4.0 * model.lbda / model.n)\n",
    "else:\n",
    "    step = 1.0 / (max_squared_sum + model.lbda / model.n)\n",
    "\n",
    "x_init = np.zeros(d)\n",
    "sag_inspector = inspector(model.loss, x_truth, verbose=True)\n",
    "x_sag = sag(x_init, iis, model.grad_i, n * n_iter, step, callback=sag_inspector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVRG\n",
    "\n",
    "We recall that an iteration of SVRG writes\n",
    "\n",
    "For $k=1, \\ldots, $ until convergence\n",
    "\n",
    "1. Set $\\tilde x \\gets \\tilde x^{(k)}$ and $x_1^{(k)} \\gets \\tilde x$\n",
    "2. Compute $\\mu_k \\gets \\nabla f(\\tilde x)$\n",
    "3. For $t=1, \\ldots, n$\n",
    "    4. Pick $i$ uniformly at random in $\\{1, \\ldots, n\\}$\n",
    "    5. Apply the step \n",
    "$$\n",
    "x_{t+1}^{(k)} \\gets x_t^{(k)} - \\eta \\big(\\nabla f_{i}(x_t^{(k)}) - \\nabla f_{i}(\\tilde x) + \\mu_k \\big) \n",
    "$$\n",
    "\n",
    "6. Set $\\tilde x^{(k+1)} \\gets x_{n+1}^{(k)}$\n",
    "\n",
    "where $\\eta$ is the step-size (see code below).\n",
    "\n",
    "*QUESTION*:\n",
    "- Fill in the iteration of the SVRG solver in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def svrg(x_init, iis, grad, grad_i, n_iter, step, callback=None):\n",
    "    \"\"\"Stochastic variance reduction gradient algorithm.\"\"\"\n",
    "    x = x_init.copy()\n",
    "    x_old = x.copy()\n",
    "    \n",
    "    for idx in range(n_iter):\n",
    "        if idx % n == 0:\n",
    "            x_old = x.copy()\n",
    "            mu = grad(x_old)\n",
    "\n",
    "        i = iis[idx]\n",
    "        \n",
    "        x = x - step * (grad_i(i, x) - grad_i(i, x_old) + mu)\n",
    "       \n",
    "        # Update metrics after each iteration.\n",
    "        if callback and idx % n == 0:\n",
    "            callback(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_squared_sum = np.max(np.sum(model.A ** 2, axis=1))\n",
    "\n",
    "if isinstance(model, LogReg):\n",
    "    step = 4.0 / (max_squared_sum + 4.0 * model.lbda / model.n)\n",
    "else:\n",
    "    step = 1.0 / (max_squared_sum + model.lbda / model.n)\n",
    "\n",
    "x_init = np.zeros(d)\n",
    "svrg_inspector = inspector(model.loss, x_truth, verbose=True)    \n",
    "x_svrg = svrg(x_init, iis, model.grad, model.grad_i, n * n_iter, step, callback=svrg_inspector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "inspectors = [sgd_inspector, sag_inspector, svrg_inspector]\n",
    "\n",
    "solvers = [\"SGD\", \"SAG\", \"SVRG\"]\n",
    "\n",
    "for insp in inspectors:\n",
    "    plt.plot(insp.obj, lw=2)\n",
    "    plt.title(\"Loss\", fontsize=18)\n",
    "    plt.xlabel(\"iteration\", fontsize=14)\n",
    "    plt.ylabel(\"$f(x_k) - f(x^*)$\", fontsize=14)\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "plt.legend(solvers)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "for insp in inspectors:\n",
    "    plt.plot(insp.err, lw=2)\n",
    "    plt.title(\"Error\", fontsize=18)\n",
    "    plt.xlabel(\"iteration\", fontsize=14)\n",
    "    plt.ylabel(\"$\\|x_k - x^*\\|_2$\", fontsize=14)\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "plt.legend(solvers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='comp'></a> \n",
    "## 5. Numerical comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "inspectors = [ista_inspector, fista_inspector, cg_inspector, bfgs_inspector,\n",
    "              sgd_inspector, sag_inspector, svrg_inspector]\n",
    "\n",
    "solvers = [\"ISTA\", \"FISTA\", \"CG\", \"BGFS\", \"SGD\", \"SAG\", \"SVRG\"]\n",
    "\n",
    "for insp in inspectors:\n",
    "    plt.plot(insp.obj, lw=2)\n",
    "    plt.title(\"Loss\", fontsize=18)\n",
    "    plt.xlabel(\"iteration\", fontsize=14)\n",
    "    plt.ylabel(\"$f(x_k) - f(x^*)$\", fontsize=14)\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "plt.legend(solvers)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "for insp in inspectors:\n",
    "    plt.plot(insp.err, lw=2)\n",
    "    plt.title(\"Distance to optimum\", fontsize=18)\n",
    "    plt.xlabel(\"iteration\", fontsize=14)\n",
    "    plt.ylabel(\"$\\|x_k - x^*\\|_2$\", fontsize=14)\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "plt.legend(solvers);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_cg\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "def compute_and_plot(model, x_truth, title=\"Optimisation algorithms comparison\", verbose=False):\n",
    "    n = model.b.shape[0]\n",
    "    d = x_truth.shape[0]\n",
    "\n",
    "    # Get a very precise minimum to compute distances to minimum\n",
    "    x_init = np.zeros(d)\n",
    "    x_min, f_min, _ = fmin_l_bfgs_b(model.loss, x_init, model.grad, pgtol=1e-20)\n",
    "    \n",
    "    def inspector(loss_fun, x_real, verbose=False):\n",
    "        \"\"\"A closure called to update metrics after each iteration.\"\"\"\n",
    "        objectives = []\n",
    "        errors = []\n",
    "        it = [0] # This is a hack to be able to modify 'it' inside the closure.\n",
    "        def inspector_cl(xk):\n",
    "            obj = loss_fun(xk) - f_min\n",
    "            err = norm(xk - x_min)\n",
    "            objectives.append(obj)\n",
    "            errors.append(err)\n",
    "            if verbose == True:\n",
    "                if it[0] == 0:\n",
    "                    print(' | '.join([name.center(8) for name in [\"it\", \"obj\", \"err\"]]))\n",
    "                if it[0] % (n_iter / 5) == 0:\n",
    "                    print(' | '.join([(\"%d\" % it[0]).rjust(8), (\"%.2e\" % obj).rjust(8), (\"%.2e\" % err).rjust(8)]))\n",
    "                it[0] += 1\n",
    "        inspector_cl.obj = objectives\n",
    "        inspector_cl.err = errors\n",
    "        return inspector_cl\n",
    "\n",
    "    ### DETERMINISTIC METHODS ###\n",
    "    n_iter = 50\n",
    "\n",
    "    # ISTA\n",
    "    step = 1. / model.lipschitz_constant()\n",
    "    x_init = np.zeros(d)\n",
    "    ista_inspector = inspector(model.loss, x_truth, verbose=verbose)\n",
    "    x_ista = ista(x_init, model.grad, n_iter, step, callback=ista_inspector)\n",
    "\n",
    "    # FISTA\n",
    "    step = 1. / model.lipschitz_constant()\n",
    "    x_init = np.zeros(d)\n",
    "    fista_inspector = inspector(model.loss, x_truth, verbose=verbose)\n",
    "    x_fista = fista(x_init, model.grad, n_iter, step, callback=fista_inspector)\n",
    "\n",
    "    # Conjugate gradient\n",
    "    x_init = np.zeros(d)\n",
    "    cg_inspector = inspector(model.loss, x_truth, verbose=verbose)\n",
    "    x_cg = fmin_cg(model.loss, x_init, model.grad, maxiter=n_iter, callback=cg_inspector, disp=verbose)\n",
    "\n",
    "    # L-BFGS-B\n",
    "    x_init = np.zeros(d)\n",
    "    bfgs_inspector = inspector(model.loss, x_truth, verbose=verbose)\n",
    "    x_bfgs, _, _ = fmin_l_bfgs_b(model.loss, x_init, model.grad, maxiter=n_iter, callback=bfgs_inspector)\n",
    "\n",
    "    ### STOCHASTIC METHODS ###\n",
    "    n_iter = 50\n",
    "\n",
    "    # generate indices of random samples\n",
    "    iis = np.random.randint(0, n, n * n_iter)  \n",
    "\n",
    "    # SGD\n",
    "    step0 = 0.1\n",
    "    x_init = np.zeros(d)\n",
    "    sgd_inspector = inspector(model.loss, x_truth, verbose=verbose)\n",
    "    x_sgd = sgd(x_init, iis, model.grad_i, n * n_iter, step=step0, callback=sgd_inspector)\n",
    "\n",
    "    # SAG\n",
    "    max_squared_sum = np.max(np.sum(model.A ** 2, axis=1))\n",
    "    if isinstance(model, LogReg):\n",
    "        step = 4.0 / (max_squared_sum + 4.0 * model.lbda / model.n)\n",
    "    else:\n",
    "        step = 1.0 / (max_squared_sum + model.lbda / model.n)\n",
    "    x_init = np.zeros(d)\n",
    "    sag_inspector = inspector(model.loss, x_truth, verbose=verbose)\n",
    "    x_sag = sag(x_init, iis, model.grad_i, n * n_iter, step, callback=sag_inspector)\n",
    "\n",
    "    # SVRG\n",
    "    max_squared_sum = np.max(np.sum(model.A ** 2, axis=1))\n",
    "    if isinstance(model, LogReg):\n",
    "        step = 4.0 / (max_squared_sum + 4.0 * model.lbda / model.n)\n",
    "    else:\n",
    "        step = 1.0 / (max_squared_sum + model.lbda / model.n)\n",
    "    x_init = np.zeros(d)\n",
    "    svrg_inspector = inspector(model.loss, x_truth, verbose=verbose)    \n",
    "    x_svrg = svrg(x_init, iis, model.grad, model.grad_i, n * n_iter, step, callback=svrg_inspector)\n",
    "\n",
    "    ### PLOT RESULTS ###\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "    inspectors = [ista_inspector, fista_inspector, cg_inspector, bfgs_inspector,\n",
    "                  sgd_inspector, sag_inspector, svrg_inspector]\n",
    "    solvers = [\"ISTA\", \"FISTA\", \"CG\", \"BGFS\", \"SGD\", \"SAG\", \"SVRG\"]\n",
    "\n",
    "    for insp in inspectors:\n",
    "        plt.plot(insp.obj, lw=2)\n",
    "        plt.title(\"Loss\", fontsize=18)\n",
    "        plt.xlabel(\"iteration\", fontsize=14)\n",
    "        plt.ylabel(\"$f(x_k) - f(x^*)$\", fontsize=14)\n",
    "        plt.yscale(\"log\")\n",
    "\n",
    "    plt.legend(solvers)\n",
    "    plt.subplot(1, 2, 2)\n",
    "\n",
    "    for insp in inspectors:\n",
    "        plt.plot(insp.err, lw=2)\n",
    "        plt.title(\"Distance to optimum\", fontsize=18)\n",
    "        plt.xlabel(\"iteration\", fontsize=14)\n",
    "        plt.ylabel(\"$\\|x_k - x^*\\|_2$\", fontsize=14)\n",
    "        plt.yscale(\"log\")\n",
    "\n",
    "    plt.legend(solvers)\n",
    "    plt.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = 50\n",
    "n = 10000\n",
    "idx = np.arange(d)\n",
    "\n",
    "# Ground truth coefficients of the model\n",
    "x_truth = (-1) ** (idx - 1) * np.exp(-idx / 10.)\n",
    "\n",
    "# Linear Regression\n",
    "for c in [0.1, 0.5, 0.7, 0.95]:\n",
    "    A, b = simu_linreg(x_truth, n, std=1., corr=c)\n",
    "    \n",
    "    # Low ridge\n",
    "    model = LinReg(A, b, 1/n)\n",
    "    compute_and_plot(model, x_truth, title=\"Linear Regression - Low Ridge - corr = \" + str(c))\n",
    "\n",
    "    # High ridge\n",
    "    model = LinReg(A, b, 1/np.sqrt(n))\n",
    "    compute_and_plot(model, x_truth, title=\"Linear Regression - High Ridge - corr = \" + str(c))\n",
    "\n",
    "# Logistic Regression\n",
    "for c in [0.1, 0.5, 0.7, 0.95]:\n",
    "    A, b = simu_logreg(x_truth, n, std=1., corr=c)\n",
    "    \n",
    "    # Low ridge\n",
    "    model = LogReg(A, b, 1/n)\n",
    "    compute_and_plot(model, x_truth, title=\"Logistic Regression - Low Ridge - corr = \" + str(c))\n",
    "\n",
    "    # High ridge\n",
    "    model = LogReg(A, b, 1/np.sqrt(n))\n",
    "    compute_and_plot(model, x_truth, title=\"Logistic Regression - High Ridge - corr = \" + str(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='conc'></a>\n",
    "## 6. Conclusion\n",
    "\n",
    "*QUESTIONS*:\n",
    "- Compare and comment your results\n",
    "- Change the value of the ridge regularization (the ``lbda`` parameter) to low ridge $\\lambda = 1 / n$ and high ridge regularization $\\lambda = 1 / \\sqrt n$ and compare your results. Comment.\n",
    "- Play also with the level of correlation between features (parameter ``corr`` above), and compare results with low and high correlation.\n",
    "- Conclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Answers_**\n",
    "* SGD performs really well on the first iterations (it beats all the other methods, even if it is less clear on the logistic regression case), but it is not good after 10 iterations. SVRG seems to be the best optimisation algorithm in many cases, since it gives a minimum with precision $10^{-10}$ in less than 20 iterations.\n",
    "* Stochastic methods perform better on linear regression (in particular SAG is really good on linear regression, but not so on logistic regression). \n",
    "* On linear regression, the regularization level does not meaningfully affect the results. However, on logistic regression, stochastic algorithms with high penalization converge quicker to the minimum (deterministic methods does not seem affected either).\n",
    "* Deterministic algorithms perform as well as stochastic ones when the features are weakly correlated (BGFS is the best choice in the case of linear regression), but SVRG remains the best choice when the correlation exceeds 0.5.\n",
    "\n",
    "**Conclusion:** Except in some particular cases, SVRG is the most efficient algorithm for minimisation. SGD is a good choice if we want to get a coarse result in very few iterations (for example if we want to find a good initialisation for another algorithm). BFGS and CG perform quite well when the features are weakly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
